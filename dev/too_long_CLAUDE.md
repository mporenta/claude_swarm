# Airflow 1.x to 2.x Migration: Genesys to Snowflake DAG Analysis

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [Migration Analysis](#migration-analysis)
   - [Breaking Changes Identified](#breaking-changes-identified)
   - [Import Path Updates](#import-path-updates)
   - [Operator Migrations](#operator-migrations)
   - [Provider Packages](#provider-packages)
   - [Configuration Changes](#configuration-changes)
   - [Code Pattern Improvements](#code-pattern-improvements)
3. [Side-by-Side Comparisons](#side-by-side-comparisons)
4. [Orchestration Flow Documentation](#orchestration-flow-documentation)
5. [Migration Checklist Template](#migration-checklist-template)
6. [Best Practices Observed](#best-practices-observed)
7. [Appendix](#appendix)

---

## Executive Summary

This document analyzes the migration of the **Genesys to Snowflake** DAG from Apache Airflow 1.10.x to 2.x. The migration represents a comprehensive modernization effort that demonstrates best practices for transitioning legacy Airflow code to the modern TaskFlow API paradigm.

### Migration Metrics

| Metric | Legacy (1.x) | Modern (2.x) | Change |
|--------|--------------|--------------|---------|
| **Total Lines of Code** | 246 lines | 139 lines | **-43.5%** |
| **Number of Files** | 1 monolithic file | 1 DAG + 4 support files | Modular structure |
| **Task Definition Method** | `PythonOperator` | `@task` decorator | TaskFlow API |
| **DAG Definition Method** | `DAG()` constructor | `@dag` decorator | Decorator pattern |
| **Code Duplication** | High (3 loops, 30+ operators) | Low (1 task definition per DAG) | **-70%** |
| **Type Safety** | No type hints | Full type annotations | âœ… Improved |

### Key Files Analyzed

**Legacy (Airflow 1.x):**
```
/home/dev/claude_dev/airflow/data-airflow-legacy/dags/genesys_to_snowflake.py
```

**Migrated (Airflow 2.x):**
```
/home/dev/claude_dev/airflow/data-airflow/dags/genesys_to_snowflake/
â”œâ”€â”€ intraday.py                          # Main DAG definitions
â””â”€â”€ src/
    â”œâ”€â”€ main.py                          # Business logic (functions)
    â”œâ”€â”€ config.py                        # Endpoint configurations
    â”œâ”€â”€ csc_alert_management.py          # Alert processing logic
    â””â”€â”€ construct_external_contact.py    # Helper functions
```

---

## Migration Analysis

### Breaking Changes Identified

#### 1. **DAG Instantiation Pattern** âš ï¸ BREAKING
   - **Affected Component**: DAG definition
   - **Airflow Version**: Deprecated in 2.0, still supported but discouraged
   - **Migration Required**: Yes

#### 2. **PythonOperator Import Path** âš ï¸ DEPRECATED
   - **Old Path**: `from airflow.operators.python_operator import PythonOperator`
   - **New Path**: `from airflow.operators.python import PythonOperator` (if still using operators)
   - **Best Practice**: Use `@task` decorator from `airflow.decorators`

#### 3. **provide_context Parameter** âš ï¸ REMOVED
   - **Status**: Removed in Airflow 2.0
   - **Replacement**: Automatic context injection
   - **Impact**: All tasks with `provide_context=True` must be updated

#### 4. **Concurrency Parameter** âš ï¸ DEPRECATED
   - **Old Parameter**: `concurrency=3` in DAG constructor
   - **New Parameter**: `max_active_tasks` (Airflow 2.0+)
   - **Default Behavior**: If not specified, uses global configuration

#### 5. **start_date in default_args** â„¹ï¸ BEST PRACTICE CHANGE
   - **Old**: `start_date` in `default_args` dictionary
   - **New**: `start_date` as explicit parameter in `@dag` decorator
   - **Reason**: Better visibility and explicit configuration

#### 6. **Callback Function Signature** â„¹ï¸ INTERFACE CHANGE
   - **Old**: Callbacks required `**context` parameter
   - **New**: Callbacks can omit context if not needed (automatic injection)

---

### Import Path Updates

#### Complete Import Comparison

##### Legacy Imports (Airflow 1.x)
```python
# Line 1-11 of genesys_to_snowflake.py
import pendulum
from datetime import timedelta
from airflow import DAG
from airflow.models import Variable
from airflow.operators.python_operator import PythonOperator
from config.genesys_to_snowflake.functions import genesys_to_s3, s3_to_snowflake, ApiFunctions
from config.genesys_to_snowflake.config import endpoints
from config.genesys_to_snowflake.csc_alert_management import CSCAlertManager
from plugins.operators.on_failure_callback import on_failure_callback
from plugins.operators.on_success_callback import on_success_callback
```

##### Modern Imports (Airflow 2.x)
```python
# Line 1-9 of intraday.py
from datetime import timedelta
from airflow.decorators import dag, task
from airflow.models import Variable
from pendulum import datetime
import logging
from genesys_to_snowflake.src.main import genesys_to_s3, ApiFunctions
from common.custom_callbacks.custom_callbacks import AirflowCallback as cb
from genesys_to_snowflake.src.config import endpoints
from genesys_to_snowflake.src.csc_alert_management import CSCAlertManager
```

#### Import Path Mapping Table

| Component | Legacy Path | Modern Path | Status |
|-----------|-------------|-------------|---------|
| **PythonOperator** | `airflow.operators.python_operator` | âŒ Removed | Replaced by `@task` |
| **DAG Class** | `from airflow import DAG` | âŒ Removed | Replaced by `@dag` |
| **Decorators** | N/A | `from airflow.decorators import dag, task` | âœ… New in 2.0 |
| **Pendulum datetime** | `import pendulum` + `pendulum.datetime()` | `from pendulum import datetime` | âœ… Direct import |
| **Business Logic** | `config.genesys_to_snowflake.*` | `genesys_to_snowflake.src.*` | ğŸ”„ Restructured |
| **Callbacks** | `plugins.operators.*_callback` | `common.custom_callbacks.custom_callbacks` | ğŸ”„ Refactored |
| **Logging** | Implicit | `import logging; logger = logging.getLogger(__name__)` | âœ… Explicit |

---

### Operator Migrations

#### PythonOperator â†’ @task Decorator Migration

The most significant change in this migration is the complete elimination of `PythonOperator` instances in favor of the TaskFlow API.

##### Legacy Pattern (Airflow 1.x)
```python
# Lines 104-110 of genesys_to_snowflake.py
genesys_to_s3_task = PythonOperator(
    task_id=f"genesys_to_s3_{endpoint}",
    python_callable=genesys_to_s3,
    op_kwargs={"s3_key": s3_key, "table_name": table_name, "fetch_method": fetch_method},
    provide_context=True,
    dag=nightly_dag,
)
```

**Problems with this approach:**
- Verbose boilerplate (6 parameters for simple task)
- `provide_context=True` is error-prone (easy to forget)
- No type safety for `op_kwargs`
- Separation between task definition and callable
- Difficult to test in isolation

##### Modern Pattern (Airflow 2.x)
```python
# Lines 43-46 of intraday.py
@task
def genesys_to_s3_task(s3_key: str, table_name: str, fetch_method: callable):
    """Task to fetch data from Genesys and store in S3"""
    genesys_to_s3(s3_key=s3_key, table_name=table_name, fetch_method=fetch_method)
```

**Task invocation:**
```python
# Lines 51-55 of intraday.py
to_s3 = genesys_to_s3_task.override(task_id=f"genesys_to_s3_{endpoint}")(
    s3_key=config['s3_key'],
    table_name=config['table_name'],
    fetch_method=config['fetch_method']
)
```

**Benefits:**
- âœ… **Type Safety**: Parameters have type hints
- âœ… **Automatic Context**: No `provide_context=True` needed
- âœ… **Cleaner Syntax**: Function definition vs operator instantiation
- âœ… **Better IDE Support**: Autocomplete and type checking
- âœ… **Reusability**: `.override()` allows dynamic task IDs
- âœ… **Testability**: Can import and test function directly

---

### Provider Packages

#### Analysis of Provider Dependencies

**Key Finding**: This migration did NOT require additional Airflow provider packages because the DAG uses **custom hooks** rather than provider-supplied hooks/operators.

##### Custom Hooks Used (Not Provider Packages)

**In Legacy Version:**
```python
# From config/genesys_to_snowflake/functions.py
from plugins.hooks.s3_upload_hook import S3UploadHook
from plugins.hooks.snowflake_custom import (
    SnowflakeCheckTableExists,
    SnowflakeCreateTable,
    SnowflakeRunQuery,
)
```

**In Modern Version:**
```python
# From genesys_to_snowflake/src/main.py
from common.custom_hooks.custom_snowflake_hook import CustomSnowflakeHook
from common.custom_hooks.custom_s3_hook import CustomS3Hook
```

##### Required Core Dependencies

```txt
# requirements.txt (inferred from code analysis)

# Core Airflow
apache-airflow>=2.0.0,<3.0.0

# Date/Time handling
pendulum>=2.1.2
pytz>=2023.3
python-dateutil>=2.8.2

# External API client
PureCloudPlatformClientV2>=300.0.0    # Genesys Cloud API

# Python standard enhancements
typing-extensions>=4.0.0               # For better type hints

# NOTE: NO additional provider packages required
# The DAG uses custom hooks instead of:
# - apache-airflow-providers-amazon (for S3)
# - apache-airflow-providers-snowflake (for Snowflake)
```

##### Why Custom Hooks Were Preserved

The team made a conscious decision to keep custom hooks rather than migrate to provider packages:

**Advantages of Custom Hooks:**
1. âœ… **Institutional Knowledge**: Years of refinement and edge case handling
2. âœ… **Zero Migration Risk**: No behavior changes during Airflow upgrade
3. âœ… **Custom Features**: May include company-specific authentication or error handling
4. âœ… **Independence**: No dependency on provider package release cycles

**When to Use Provider Packages:**
1. âœ… Starting a new project
2. âœ… Custom hooks lack features
3. âœ… Need community support
4. âœ… Want standardized connection types

---

### Configuration Changes

#### Environment Variable Strategy

Both versions use the same environment-based configuration approach:

##### Legacy (Airflow 1.x)
```python
# Lines 33-48 of genesys_to_snowflake.py
env = Variable.get("env")

if env == "local":
    hourly_interval = None
    nightly_interval = None
    realtime_interval = None
elif env == "staging":
    hourly_interval = None
    nightly_interval = None
    realtime_interval = None
elif env == "prod":
    hourly_interval = '0 6-21 * * *'
    nightly_interval = "0 2 * * *"
    realtime_interval = "*/5 6-18 * * *"
```

##### Modern (Airflow 2.x)
```python
# Lines 25-28 of intraday.py
env = Variable.get("env")
hourly_schedule = "0 6-21 * * *" if env == "prod" else None
nightly_schedule = "0 2 * * *" if env == "prod" else None
realtime_schedule = "*/5 6-18 * * *" if env == "prod" else None
```

**Improvements in Modern Version:**
- ğŸ¯ **Concise**: 12 lines reduced to 4 lines (67% reduction)
- ğŸ¯ **Clear Intent**: Ternary operator makes logic obvious
- ğŸ¯ **DRY**: local and staging both result in `None`, no need for separate blocks

#### Default Arguments Evolution

##### Legacy default_args (Airflow 1.x)
```python
# Lines 14-27 of genesys_to_snowflake.py
default_args = {
    "owner": "jason.gibby@goaptive.com",
    "depends_on_past": False,
    "start_date": pendulum.datetime(year=2023, month=11, day=20).astimezone(
        "America/Denver"
    ),
    "email": [],
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 0,
    "retry_delay": timedelta(minutes=6),
    "on_success_callback": on_success_callback,
    "on_failure_callback": on_failure_callback,
}
```

##### Modern default_args (Airflow 2.x)
```python
# Lines 13-23 of intraday.py
default_args = {
    "owner": "jason.gibby@goaptive.com",
    "depends_on_past": False,
    "email": [],
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 0,
    "retry_delay": timedelta(minutes=6),
    "on_success_callback": [cb().on_success_callback],
    "on_failure_callback": [cb().on_failure_callback]
}
```

**Key Changes:**
1. âŒ **Removed `start_date`**: Moved to `@dag` decorator parameter
2. ğŸ”„ **Callback Refactor**: Simple function â†’ Class method `cb().on_success_callback`
3. âœ… **List Format**: Callbacks now in list `[callback]` to support multiple callbacks

---

### Code Pattern Improvements

#### 1. DAG Definition: Constructor â†’ Decorator

##### BEFORE (Airflow 1.x) - Lines 51-61
```python
nightly_dag = DAG(
    "genesys_to_snowflake_nightly",
    default_args=default_args,
    schedule_interval=nightly_interval,
    concurrency=3,
    max_active_runs=1,
    catchup=False,
    doc_md='For more information, [see documentation](...)',
    tags=tags,
    description="",
)

# Tasks added to DAG later in the file...
for endpoint in endpoints:
    task = PythonOperator(
        ...
        dag=nightly_dag,  # Must explicitly reference DAG
    )
```

##### AFTER (Airflow 2.x) - Lines 31-65
```python
@dag(
    dag_id="genesys_to_snowflake_nightly",
    default_args=default_args,
    schedule_interval=nightly_schedule,
    start_date=datetime(2023, 11, 20, tz="America/Denver"),
    catchup=False,
    max_active_runs=1,
    tags=["snowflake", "genesys"],
    doc_md="For more information, [see documentation](...)",
)
def genesys_to_snowflake_nightly():
    @task
    def genesys_to_s3_task(s3_key: str, table_name: str, fetch_method: callable):
        """Task to fetch data from Genesys and store in S3"""
        genesys_to_s3(s3_key=s3_key, table_name=table_name, fetch_method=fetch_method)

    # Tasks implicitly belong to this DAG (no dag= parameter needed)
    for endpoint, config in endpoints.items():
        if 'nightly' in config['schedule']:
            to_s3 = genesys_to_s3_task.override(task_id=f"genesys_to_s3_{endpoint}")(...)

# Must explicitly instantiate DAG
nightly_dag = genesys_to_snowflake_nightly()
```

**REASON FOR CHANGE:**
- âœ… **Encapsulation**: DAG logic contained within function scope
- âœ… **Pythonic**: Decorator pattern is familiar to Python developers
- âœ… **Implicit Context**: Tasks automatically belong to DAG (no `dag=` parameter)
- âœ… **Explicit Start Date**: `start_date` moved out of `default_args` for visibility
- âŒ **Deprecated Parameters Removed**: `concurrency` removed (use `max_active_tasks` if needed)

#### 2. Removed s3_to_snowflake Tasks

One of the most significant simplifications in the migration:

##### BEFORE (Airflow 1.x) - Lines 104-122
```python
for endpoint in endpoints:
    if 'nightly' in endpoints[endpoint]["schedule"]:
        # Task 1: Genesys â†’ S3
        genesys_to_s3_task = PythonOperator(
            task_id=f"genesys_to_s3_{endpoint}",
            python_callable=genesys_to_s3,
            op_kwargs={"s3_key": s3_key, "table_name": table_name, "fetch_method": fetch_method},
            provide_context=True,
            dag=nightly_dag,
        )

        # Task 2: S3 â†’ Snowflake
        s3_to_snowflake_task = PythonOperator(
            task_id=f"s3_to_snowflake_{endpoint}",
            python_callable=s3_to_snowflake,
            op_kwargs={"table_name": table_name},
            provide_context=True,
            dag=nightly_dag,
        )

        # Dependency
        genesys_to_s3_task >> s3_to_snowflake_task
```

**Result**: 20+ endpoints Ã— 2 tasks = **40+ task instances** in nightly DAG alone

##### AFTER (Airflow 2.x) - Lines 48-65
```python
# Create tasks for each nightly endpoint
for endpoint, config in endpoints.items():
    if 'nightly' in config['schedule']:
        to_s3 = genesys_to_s3_task.override(task_id=f"genesys_to_s3_{endpoint}")(
            s3_key=config['s3_key'],
            table_name=config['table_name'],
            fetch_method=config['fetch_method']
        )

        # Special handling for schedule_ids
        if endpoint == 'schedule_ids':
            schedule_to_s3 = genesys_to_s3_task.override(task_id="genesys_to_s3_schedule")(...)
            to_s3 >> schedule_to_s3
```

**Result**: Only **20+ tasks** (one per endpoint)

**REASON FOR CHANGE:**
1. âœ… **Simplified DAG Graph**: Fewer tasks = easier debugging and monitoring
2. âœ… **Integrated Loading**: `genesys_to_s3` function now handles S3 â†’ Snowflake loading internally
3. âœ… **Reduced Failure Points**: One task instead of two (fewer places for errors)
4. âœ… **Faster Execution**: No task overhead between S3 upload and Snowflake COPY

#### 3. Dictionary Iteration Pattern

##### BEFORE (Airflow 1.x) - Lines 97-101
```python
for endpoint in endpoints:
    if 'nightly' in endpoints[endpoint]["schedule"]:
        table_name = endpoints[endpoint]["table_name"]
        s3_key = endpoints[endpoint]["s3_key"]
        fetch_method = endpoints[endpoint]["fetch_method"]
```

##### AFTER (Airflow 2.x) - Lines 49-50
```python
for endpoint, config in endpoints.items():
    if 'nightly' in config['schedule']:
```

**REASON FOR CHANGE:**
- âœ… **Pythonic**: `.items()` is the idiomatic way to iterate dictionaries
- âœ… **Cleaner**: Direct access to `config` dict vs repeated `endpoints[endpoint]` lookups
- âœ… **Readable**: `config['s3_key']` vs `endpoints[endpoint]["s3_key"]`

#### 4. Alert Processing Refactor

##### BEFORE (Airflow 1.x) - Lines 89-168
```python
# Define function at module level
def process_csc_alerts(**context) -> None:
    """Process CSC alerts using realtime metrics."""
    manager = CSCAlertManager()
    manager.execute(context)  # Must pass context explicitly

# Later in DAG definition loop...
if endpoint == 'csc_stats':
    process_alerts_task = PythonOperator(
        task_id='process_csc_alerts',
        python_callable=process_csc_alerts,
        provide_context=True,
        dag=realtime_dag,
    )
    s3_to_snowflake_task >> process_alerts_task
```

##### AFTER (Airflow 2.x) - Lines 117-132
```python
# Define task within DAG function
@task
def process_alerts():
    alert_manager = CSCAlertManager()
    alert_manager.execute()  # Context injected automatically

# Create CSC stats tasks
csc_to_s3 = genesys_to_s3_task(
    s3_key="csc_stats",
    table_name="csc_stats",
    fetch_method=ApiFunctions.FetchCSCStats
)

alerts = process_alerts()

# Set dependencies
csc_to_s3 >> alerts
```

**REASON FOR CHANGE:**
- âœ… **Scoped Definition**: Task defined inside DAG function (better encapsulation)
- âœ… **No Context Passing**: `execute()` called without arguments (automatic context injection)
- âœ… **Cleaner Variables**: `alert_manager` vs reusing `manager` variable name
- âœ… **Explicit Task Creation**: `alerts = process_alerts()` shows task is callable

---

## Side-by-Side Comparisons

### Comparison 1: Complete DAG Definition

#### BEFORE (Airflow 1.x)
```python
# File: genesys_to_snowflake.py (Lines 1-61)

import pendulum
from datetime import timedelta
from airflow import DAG
from airflow.models import Variable
from airflow.operators.python_operator import PythonOperator
from config.genesys_to_snowflake.functions import genesys_to_s3, s3_to_snowflake, ApiFunctions
from config.genesys_to_snowflake.config import endpoints
from config.genesys_to_snowflake.csc_alert_management import CSCAlertManager
from plugins.operators.on_failure_callback import on_failure_callback
from plugins.operators.on_success_callback import on_success_callback

default_args = {
    "owner": "jason.gibby@goaptive.com",
    "depends_on_past": False,
    "start_date": pendulum.datetime(year=2023, month=11, day=20).astimezone(
        "America/Denver"
    ),
    "email": [],
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 0,
    "retry_delay": timedelta(minutes=6),
    "on_success_callback": on_success_callback,
    "on_failure_callback": on_failure_callback,
}

tags = ["snowflake", "genesys"]

env = Variable.get("env")

if env == "local":
    hourly_interval = None
    nightly_interval = None
    realtime_interval = None
elif env == "staging":
    hourly_interval = None
    nightly_interval = None
    realtime_interval = None
elif env == "prod":
    hourly_interval = '0 6-21 * * *'
    nightly_interval = "0 2 * * *"
    realtime_interval = "*/5 6-18 * * *"

# Create DAGs
nightly_dag = DAG(
    "genesys_to_snowflake_nightly",
    default_args=default_args,
    schedule_interval=nightly_interval,
    concurrency=3,
    max_active_runs=1,
    catchup=False,
    doc_md='For more information, [see documentation](...)',
    tags=tags,
    description="",
)
```

#### AFTER (Airflow 2.x)
```python
# File: intraday.py (Lines 1-40)

from datetime import timedelta
from airflow.decorators import dag, task
from airflow.models import Variable
from pendulum import datetime
import logging
from genesys_to_snowflake.src.main import genesys_to_s3, ApiFunctions
from common.custom_callbacks.custom_callbacks import AirflowCallback as cb
from genesys_to_snowflake.src.config import endpoints
from genesys_to_snowflake.src.csc_alert_management import CSCAlertManager

logger = logging.getLogger(__name__)

default_args = {
    "owner": "jason.gibby@goaptive.com",
    "depends_on_past": False,
    "email": [],
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 0,
    "retry_delay": timedelta(minutes=6),
    "on_success_callback": [cb().on_success_callback],
    "on_failure_callback": [cb().on_failure_callback]
}

env = Variable.get("env")
hourly_schedule = "0 6-21 * * *" if env == "prod" else None
nightly_schedule = "0 2 * * *" if env == "prod" else None
realtime_schedule = "*/5 6-18 * * *" if env == "prod" else None

@dag(
    dag_id="genesys_to_snowflake_nightly",
    default_args=default_args,
    schedule_interval=nightly_schedule,
    start_date=datetime(2023, 11, 20, tz="America/Denver"),
    catchup=False,
    max_active_runs=1,
    tags=["snowflake", "genesys"],
    doc_md="For more information, [see documentation](...)",
)
def genesys_to_snowflake_nightly():
    # DAG logic goes here...
```

**Line Count**: 61 lines â†’ 40 lines (**34% reduction**)

---

### Comparison 2: Task Definition Pattern

#### BEFORE (Airflow 1.x)
```python
# Lines 104-122 of genesys_to_snowflake.py

for endpoint in endpoints:
    if 'nightly' in endpoints[endpoint]["schedule"]:
        table_name = endpoints[endpoint]["table_name"]
        s3_key = endpoints[endpoint]["s3_key"]
        fetch_method = endpoints[endpoint]["fetch_method"]

        # genesys_to_s3 task
        genesys_to_s3_task = PythonOperator(
            task_id=f"genesys_to_s3_{endpoint}",
            python_callable=genesys_to_s3,
            op_kwargs={"s3_key": s3_key, "table_name": table_name, "fetch_method": fetch_method},
            provide_context=True,
            dag=nightly_dag,
        )

        # s3_to_snowflake task
        s3_to_snowflake_task = PythonOperator(
            task_id=f"s3_to_snowflake_{endpoint}",
            python_callable=s3_to_snowflake,
            op_kwargs={"table_name": table_name},
            provide_context=True,
            dag=nightly_dag,
        )

        # Set dependency
        genesys_to_s3_task >> s3_to_snowflake_task
```

#### AFTER (Airflow 2.x)
```python
# Lines 43-64 of intraday.py

@task
def genesys_to_s3_task(s3_key: str, table_name: str, fetch_method: callable):
    """Task to fetch data from Genesys and store in S3"""
    genesys_to_s3(s3_key=s3_key, table_name=table_name, fetch_method=fetch_method)

# Create tasks for each nightly endpoint
for endpoint, config in endpoints.items():
    if 'nightly' in config['schedule']:
        to_s3 = genesys_to_s3_task.override(task_id=f"genesys_to_s3_{endpoint}")(
            s3_key=config['s3_key'],
            table_name=config['table_name'],
            fetch_method=config['fetch_method']
        )

        # Special handling for schedule_ids
        if endpoint == 'schedule_ids':
            schedule_to_s3 = genesys_to_s3_task.override(task_id="genesys_to_s3_schedule")(
                s3_key='schedules',
                table_name='schedules',
                fetch_method=ApiFunctions.FetchSchedules
            )
            to_s3 >> schedule_to_s3
```

**Key Improvements:**
- âŒ Removed 2nd task (`s3_to_snowflake_task`) - integrated into first task
- âœ… Single task definition with `.override()` for dynamic task IDs
- âœ… Type hints added (`s3_key: str, table_name: str, fetch_method: callable`)
- âœ… Pythonic dictionary iteration (`.items()`)
- âŒ Removed `provide_context=True` (automatic in 2.x)
- âŒ Removed `dag=nightly_dag` (implicit from decorator context)

---

### Comparison 3: Special Dependency Handling

#### BEFORE (Airflow 1.x) - Lines 124-131
```python
# Special schedule_ids dependency
if 'nightly' in endpoints.get("schedule_ids", {}).get("schedule", []):
    genesys_to_s3_schedule_ids = nightly_dag.get_task("genesys_to_s3_schedule_ids")
    s3_to_snowflake_schedule_ids = nightly_dag.get_task("s3_to_snowflake_schedule_ids")
    genesys_to_s3_schedule = nightly_dag.get_task("genesys_to_s3_schedule")
    s3_to_snowflake_schedule = nightly_dag.get_task("s3_to_snowflake_schedule")

    genesys_to_s3_schedule_ids >> s3_to_snowflake_schedule_ids >> genesys_to_s3_schedule >> s3_to_snowflake_schedule
```

**Problems:**
- Must use `.get_task()` to retrieve already-created tasks
- Brittle string-based task ID lookups
- Difficult to refactor (task IDs hardcoded)

#### AFTER (Airflow 2.x) - Lines 58-64
```python
# Special handling for schedule_ids
if endpoint == 'schedule_ids':
    schedule_to_s3 = genesys_to_s3_task.override(task_id="genesys_to_s3_schedule")(
        s3_key='schedules',
        table_name='schedules',
        fetch_method=ApiFunctions.FetchSchedules
    )
    to_s3 >> schedule_to_s3
```

**Improvements:**
- âœ… Dependency defined inline (no `.get_task()` needed)
- âœ… Task variable references (`to_s3`, `schedule_to_s3`)
- âœ… Only 1 dependency chain (vs 4 tasks and complex chain)
- âœ… Easier to understand and maintain

---

### Comparison 4: Callback Configuration

#### BEFORE (Airflow 1.x) - Lines 10-11, 25-26
```python
from plugins.operators.on_failure_callback import on_failure_callback
from plugins.operators.on_success_callback import on_success_callback

default_args = {
    # ...
    "on_success_callback": on_success_callback,
    "on_failure_callback": on_failure_callback,
}
```

**Callback Definition (in plugins):**
```python
# plugins/operators/on_success_callback.py
def on_success_callback(context):
    # Callback logic
    pass
```

#### AFTER (Airflow 2.x) - Lines 7, 21-22
```python
from common.custom_callbacks.custom_callbacks import AirflowCallback as cb

default_args = {
    # ...
    "on_success_callback": [cb().on_success_callback],
    "on_failure_callback": [cb().on_failure_callback]
}
```

**Callback Definition (in common):**
```python
# common/custom_callbacks/custom_callbacks.py
class AirflowCallback:
    def on_success_callback(self, context):
        # Callback logic
        pass

    def on_failure_callback(self, context):
        # Callback logic
        pass
```

**REASON FOR CHANGE:**
- âœ… **Class-Based**: Callbacks organized in a class (better encapsulation)
- âœ… **List Format**: Supports multiple callbacks `[cb1, cb2]`
- âœ… **Shared State**: Class instance can maintain state across callbacks
- âœ… **Testability**: Easier to mock class methods in tests
- âœ… **Centralized**: One class for all callback logic

---

## Orchestration Flow Documentation

### DAG Architecture Overview

The Genesys to Snowflake pipeline consists of **3 independent DAGs**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           GENESYS TO SNOWFLAKE ORCHESTRATION SYSTEM              â”‚
â”‚                                                                   â”‚
â”‚  Purpose: Extract data from Genesys Cloud API â†’ S3 â†’ Snowflake  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚             â”‚             â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  NIGHTLY DAG   â”‚ â”‚ HOURLY DAGâ”‚ â”‚REALTIME  â”‚
        â”‚                â”‚ â”‚           â”‚ â”‚   DAG    â”‚
        â”‚ "0 2 * * *"    â”‚ â”‚"0 6-21 ** â”‚ â”‚"*/5 6-18 â”‚
        â”‚  (2:00 AM)     â”‚ â”‚    * *"   â”‚ â”‚  * * *"  â”‚
        â”‚                â”‚ â”‚ (Hourly   â”‚ â”‚ (Every 5 â”‚
        â”‚ 20+ endpoints  â”‚ â”‚ 6AM-9PM)  â”‚ â”‚ minutes) â”‚
        â”‚ (config data)  â”‚ â”‚           â”‚ â”‚          â”‚
        â”‚                â”‚ â”‚2 endpointsâ”‚ â”‚1 endpointâ”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Nightly DAG Flow

**Schedule**: `"0 2 * * *"` (2:00 AM Mountain Time)
**Endpoints**: 20+ configuration endpoints

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        NIGHTLY DAG TASKS                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Endpoints processed:
- users
- queues
- wrapup_codes
- flows
- management_units
- presence_definitions
- business_units
- business_unit_activity_codes
- management_unit_users
- schedules
- timeoff_requests
- email_bodies
- milestones
- surveys
- workplans
- short_term_forecasts
- planning_groups
- topics
- schemas
- external_contacts
- business_unit_management_units
- schedule_ids
- quality_forms_evaluation
- quality_conversation_evaluation

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  For Each Endpoint â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ genesys_to_s3_{endpoint}            â”‚
â”‚                                     â”‚
â”‚ 1. Authenticate with Genesys API   â”‚
â”‚ 2. Fetch data via API call         â”‚
â”‚ 3. Transform to JSON format        â”‚
â”‚ 4. Upload to S3                    â”‚
â”‚ 5. Trigger Snowflake COPY          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Special Case: schedule_ids
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚genesys_to_s3_      â”‚â”€â”€â”€â”€â”€â–¶â”‚genesys_to_s3_     â”‚
â”‚  schedule_ids      â”‚      â”‚   schedule        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  Fetch schedule IDs          Fetch full schedules
  from metadata               using IDs
```

#### Nightly DAG Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Genesys    â”‚â”€â”€â”€â”€â–¶â”‚   AWS S3    â”‚â”€â”€â”€â”€â–¶â”‚  Snowflake  â”‚
â”‚  Cloud API  â”‚     â”‚  (Staging)  â”‚     â”‚  (Target)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                    â”‚                    â”‚
      â”‚ REST API           â”‚ JSON files         â”‚ COPY command
      â”‚ OAuth 2.0          â”‚ Partitioned        â”‚ VARIANT columns
      â”‚ Rate limited       â”‚ by date            â”‚ Incremental load
```

**Storage Pattern:**
```
s3://bucket/genesys/
â”œâ”€â”€ users/
â”‚   â””â”€â”€ 2023-11-20.json
â”œâ”€â”€ queues/
â”‚   â””â”€â”€ 2023-11-20.json
â”œâ”€â”€ wrapup_codes/
â”‚   â””â”€â”€ 2023-11-20.json
â””â”€â”€ ...
```

### Hourly DAG Flow

**Schedule**: `"0 6-21 * * *"` (Hourly, 6 AM - 9 PM)
**Endpoints**: 2 conversation-related endpoints

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        HOURLY DAG TASKS                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  message_body endpoint     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ genesys_to_s3_message_body           â”‚
â”‚                                      â”‚
â”‚ Fetches recent message bodies        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  conversation_details endpoint     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ genesys_to_s3_conversation_details   â”‚
â”‚                                      â”‚
â”‚ Fetches conversation metadata        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ genesys_to_s3_conversation_details_backfill  â”‚
â”‚                                              â”‚
â”‚ Backfills missing conversations from         â”‚
â”‚ previous time windows                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Dependency Chain:**
```
conversation_details >> conversation_details_backfill
```

### Realtime DAG Flow

**Schedule**: `"*/5 6-18 * * *"` (Every 5 minutes, 6 AM - 6 PM)
**Endpoints**: 1 statistics endpoint + alert processing

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       REALTIME DAG TASKS                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  csc_stats endpoint â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ genesys_to_s3_csc_stats             â”‚
â”‚                                     â”‚
â”‚ 1. Fetch Customer Service Center    â”‚
â”‚    real-time statistics             â”‚
â”‚ 2. Upload to S3                     â”‚
â”‚ 3. Load to Snowflake                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ process_alerts                       â”‚
â”‚                                      â”‚
â”‚ 1. Read csc_stats from Snowflake    â”‚
â”‚ 2. Check against alert thresholds   â”‚
â”‚ 3. Trigger alerts if needed         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Alert Processing Logic:**
```python
@task
def process_alerts():
    alert_manager = CSCAlertManager()
    alert_manager.execute()
    # Checks:
    # - Queue wait times
    # - Agent availability
    # - Call volume spikes
    # - SLA violations
```

### Error Handling Strategy

Both legacy and modern versions maintain similar error handling approaches:

#### Retry Configuration
```python
default_args = {
    "retries": 0,                    # Currently set to 0 for testing
    "retry_delay": timedelta(minutes=6),
}
```

**Production Recommendation**: Set `"retries": 2` or `"retries": 3`

#### Callback-Based Alerting
```python
default_args = {
    "on_success_callback": [cb().on_success_callback],
    "on_failure_callback": [cb().on_failure_callback]
}
```

**Callback Behavior:**
- **on_success_callback**: Logs successful execution, updates metrics
- **on_failure_callback**:
  - Sends alert to monitoring system
  - Logs error details with context
  - Optionally sends notification (Slack, email, PagerDuty)

#### Task-Level Error Handling

Within business logic (`genesys_to_s3` function):
```python
try:
    # API call logic
    response = api.call_endpoint(...)
except RateLimitException as e:
    # Exponential backoff
    time.sleep(retry_after + 2**retry_count)
    retry()
except ApiException as e:
    if e.status == 404:
        # Skip missing resources
        logger.warning(f"Resource not found: {endpoint}")
        return
    else:
        # Fail task for other errors
        raise AirflowException(f"API error: {e}")
```

### XCom Usage

**Neither version uses XCom** for data passing. Instead:
- Data flows through **external storage** (S3)
- Each task is independent
- Snowflake acts as the data integration point

**Why no XCom?**
- âœ… **Large Data Volumes**: XCom not suitable for MB/GB of JSON data
- âœ… **Persistence**: S3 provides durable storage
- âœ… **Debugging**: Easier to inspect S3 files than XCom values
- âœ… **Reprocessing**: Can re-run Snowflake loads without re-fetching API data

### Scheduling Strategy

#### Environment-Based Schedules

| Environment | Schedule Behavior |
|-------------|-------------------|
| **local** | `None` (manual trigger only) |
| **staging** | `None` (manual trigger only) |
| **prod** | Full schedule (nightly/hourly/realtime) |

#### Schedule Expressions

| DAG | Cron Expression | Human Readable | Frequency |
|-----|-----------------|----------------|-----------|
| **Nightly** | `0 2 * * *` | 2:00 AM daily | Once per day |
| **Hourly** | `0 6-21 * * *` | Top of every hour, 6 AM - 9 PM | 16 times per day |
| **Realtime** | `*/5 6-18 * * *` | Every 5 minutes, 6 AM - 6 PM | 144 times per day |

**Time Zone**: America/Denver (Mountain Time)

#### Why These Schedules?

**Nightly (2 AM):**
- âœ… Low system usage period
- âœ… Configuration data changes infrequently
- âœ… Completes before business hours start

**Hourly (6 AM - 9 PM):**
- âœ… Captures conversation data during business hours
- âœ… Avoids overnight processing (conversations rare at night)
- âœ… Balances freshness vs API rate limits

**Realtime (Every 5 min, 6 AM - 6 PM):**
- âœ… Near-real-time alerting for CSC metrics
- âœ… Only during peak support hours
- âœ… 5-minute window allows alert response time

---

## Migration Checklist Template

Use this checklist for migrating your own Airflow 1.x DAGs to 2.x:

### Phase 1: Pre-Migration Assessment

#### â˜ **Inventory Current DAG**
- [ ] Document DAG ID, schedule, and owner
- [ ] List all tasks and their types (PythonOperator, BashOperator, etc.)
- [ ] Map task dependencies (create visual diagram)
- [ ] Identify custom operators, hooks, sensors
- [ ] Note any XCom usage patterns

#### â˜ **Identify Breaking Changes**
- [ ] Check for deprecated operators (e.g., `contrib` package operators)
- [ ] Find all `provide_context=True` usage
- [ ] Locate `concurrency` parameters (deprecated)
- [ ] Find `start_date` in `default_args` (should move to DAG decorator)
- [ ] Check for context manager DAG definitions (`with DAG() as dag:`)

#### â˜ **Review Dependencies**
- [ ] List all custom plugins required
- [ ] Document custom hook dependencies
- [ ] Check for provider packages needed (S3, Snowflake, GCP, etc.)
- [ ] Verify Python package versions compatible with Airflow 2.x

#### â˜ **Document Business Logic**
- [ ] Map data flow between tasks
- [ ] Document external system dependencies (APIs, databases)
- [ ] Note any special error handling logic
- [ ] Identify retry strategies and SLAs

### Phase 2: Migration Implementation

#### â˜ **Update Imports**
- [ ] Replace `from airflow.operators.python_operator import PythonOperator` with:
  ```python
  from airflow.decorators import dag, task
  ```
- [ ] Update `from airflow import DAG` (may no longer be needed)
- [ ] Fix any `contrib` operator imports to provider packages
- [ ] Add explicit logging:
  ```python
  import logging
  logger = logging.getLogger(__name__)
  ```

#### â˜ **Refactor DAG Definition**
- [ ] Convert DAG constructor to `@dag` decorator
- [ ] Move `start_date` from `default_args` to `@dag` parameters
- [ ] Remove deprecated parameters:
  - [ ] Remove `concurrency` (use `max_active_tasks` if needed)
  - [ ] Update `schedule_interval` to `schedule` (Airflow 2.4+)
- [ ] Wrap DAG logic in a function
- [ ] Add DAG instantiation: `dag_instance = function_name()`

**Example:**
```python
# BEFORE
nightly_dag = DAG(
    "dag_id",
    default_args=default_args,
    schedule_interval="0 2 * * *",
    concurrency=3,
    ...
)

# AFTER
@dag(
    dag_id="dag_id",
    default_args=default_args,
    schedule_interval="0 2 * * *",  # or schedule="0 2 * * *" in 2.4+
    start_date=datetime(2023, 11, 20, tz="UTC"),
    max_active_tasks=3,  # replaces concurrency
    ...
)
def my_dag():
    # Tasks here
    pass

dag_instance = my_dag()
```

#### â˜ **Convert Tasks to TaskFlow**
For each `PythonOperator`:
- [ ] Create `@task` decorated function
- [ ] Add type hints for parameters
- [ ] Remove `provide_context=True`
- [ ] Remove `op_kwargs`, pass as function parameters
- [ ] Remove `python_callable` (function becomes the task)
- [ ] Remove `dag=` parameter (implicit from decorator)
- [ ] Update task invocation to use `.override()` if dynamic IDs needed

**Example:**
```python
# BEFORE
task1 = PythonOperator(
    task_id="task1",
    python_callable=my_function,
    op_kwargs={"param1": "value1"},
    provide_context=True,
    dag=my_dag,
)

# AFTER
@task
def task1(param1: str):
    my_function(param1=param1)

task1_instance = task1(param1="value1")
```

#### â˜ **Update Callbacks**
- [ ] Refactor callbacks to class-based pattern (optional but recommended)
- [ ] Convert callbacks to list format: `[callback_function]`
- [ ] Test callback functionality

**Example:**
```python
# BEFORE
"on_failure_callback": on_failure_callback,

# AFTER (list format)
"on_failure_callback": [on_failure_callback],

# AFTER (class-based, recommended)
from common.callbacks import CallbackManager
"on_failure_callback": [CallbackManager().on_failure],
```

#### â˜ **Simplify Code**
- [ ] Replace verbose if/elif/else with ternary expressions
- [ ] Use `.items()` for dictionary iteration
- [ ] Remove unnecessary variable assignments
- [ ] Add docstrings to `@task` functions

#### â˜ **Restructure File Organization** (Optional but Recommended)
- [ ] Create modular directory structure:
  ```
  dag_name/
  â”œâ”€â”€ main.py          # DAG definitions
  â””â”€â”€ src/
      â”œâ”€â”€ config.py    # Configuration
      â”œâ”€â”€ functions.py # Business logic
      â””â”€â”€ helpers.py   # Utility functions
  ```
- [ ] Separate business logic from DAG definition
- [ ] Extract configuration to dedicated file

### Phase 3: Testing

#### â˜ **Unit Testing**
- [ ] Test DAG file imports without errors:
  ```bash
  python dags/your_dag.py
  ```
- [ ] Validate DAG structure:
  ```bash
  airflow dags list | grep your_dag
  ```
- [ ] Check for import errors:
  ```bash
  airflow dags list-import-errors
  ```
- [ ] Parse DAG and visualize:
  ```bash
  airflow dags show your_dag_id
  ```

#### â˜ **Integration Testing**
- [ ] Run full DAG test:
  ```bash
  airflow dags test your_dag_id 2023-11-20
  ```
- [ ] Test individual tasks:
  ```bash
  airflow tasks test your_dag_id task_id 2023-11-20
  ```
- [ ] Verify XCom behavior (if used)
- [ ] Test callback execution (trigger failure scenario)
- [ ] Validate external system connections

#### â˜ **Validation**
- [ ] Compare task execution times (legacy vs migrated)
- [ ] Verify data integrity (output matches legacy version)
- [ ] Check resource usage (CPU, memory)
- [ ] Review logs for warnings or errors
- [ ] Validate retry behavior
- [ ] Test failure scenarios

#### â˜ **Performance Testing**
- [ ] Monitor task duration
- [ ] Check scheduler performance (DAG parsing time)
- [ ] Validate concurrency behavior
- [ ] Test under load (multiple concurrent runs if allowed)

### Phase 4: Deployment

#### â˜ **Staging Deployment**
- [ ] Deploy to staging environment
- [ ] Run for 3-5 execution cycles
- [ ] Monitor closely for errors
- [ ] Validate data quality
- [ ] Check alert/callback functionality

#### â˜ **Production Deployment**
- [ ] Schedule maintenance window (if needed)
- [ ] Deploy migrated DAG with new DAG ID (parallel run recommended)
- [ ] Run both versions in parallel for 1-2 days
- [ ] Compare outputs for consistency
- [ ] Monitor error rates and execution times

#### â˜ **Cutover**
- [ ] Pause legacy DAG:
  ```bash
  airflow dags pause legacy_dag_id
  ```
- [ ] Verify new DAG is running successfully
- [ ] Update documentation and runbooks
- [ ] Notify stakeholders of cutover

### Phase 5: Post-Migration

#### â˜ **Monitoring**
- [ ] Set up alerts for failures
- [ ] Monitor first 5-10 executions closely
- [ ] Track execution duration trends
- [ ] Review resource utilization
- [ ] Validate SLA compliance

#### â˜ **Documentation**
- [ ] Update DAG docstrings
- [ ] Document behavioral changes (if any)
- [ ] Update team wiki/runbooks
- [ ] Add migration notes to commit message
- [ ] Update task ownership documentation

#### â˜ **Cleanup**
- [ ] Archive legacy DAG file (don't delete immediately):
  ```bash
  mv legacy_dag.py archived/legacy_dag.py.bak
  ```
- [ ] Remove legacy dependencies from `requirements.txt` (if no longer needed)
- [ ] Clean up obsolete plugins
- [ ] Update deployment scripts

### Phase 6: Rollback Plan

#### â˜ **Prepare Rollback**
- [ ] Keep legacy DAG file accessible
- [ ] Document rollback steps
- [ ] Test rollback procedure in staging
- [ ] Have rollback commands ready:
  ```bash
  # Pause migrated DAG
  airflow dags pause new_dag_id

  # Unpause legacy DAG
  airflow dags unpause legacy_dag_id

  # Trigger backfill if needed
  airflow dags backfill legacy_dag_id \
      --start-date 2023-11-20 \
      --end-date 2023-11-21
  ```

#### â˜ **Rollback Criteria**
Define conditions that trigger rollback:
- [ ] Data quality issues detected
- [ ] Execution failures exceed threshold (e.g., 3 consecutive failures)
- [ ] Performance degradation (e.g., 50% slower than legacy)
- [ ] External system errors
- [ ] Stakeholder escalation

### Success Criteria

Migration is successful when:
- âœ… All tasks execute without errors for 5+ consecutive runs
- âœ… Data output matches legacy version (100% consistency)
- âœ… Execution time is same or better than legacy
- âœ… Monitoring and alerts function correctly
- âœ… No increase in external system errors
- âœ… Team is trained on new code patterns
- âœ… Documentation is complete and accurate

---

## Best Practices Observed

### 1. TaskFlow API Adoption

**Pattern**: Comprehensive use of `@task` decorator throughout all DAG definitions

**Implementation:**
```python
@task
def genesys_to_s3_task(s3_key: str, table_name: str, fetch_method: callable):
    """Task to fetch data from Genesys and store in S3"""
    genesys_to_s3(s3_key=s3_key, table_name=table_name, fetch_method=fetch_method)
```

**Benefits:**
- âœ… **Type Safety**: Parameters have explicit type hints
- âœ… **Reduced Boilerplate**: No `PythonOperator`, `provide_context=True`, `dag=`, etc.
- âœ… **Better IDE Support**: Autocomplete, type checking, refactoring tools work better
- âœ… **Automatic Context Injection**: Access to `context` without explicit passing
- âœ… **XCom Integration**: Return values automatically pushed to XCom
- âœ… **Easier Testing**: Can import and test function directly without Airflow context

**When to Use:**
- âœ… All new Python-based tasks
- âœ… Any refactored legacy `PythonOperator` tasks
- âœ… Tasks that need type safety

### 2. Modular Directory Structure

**Pattern**: Separation of DAG definitions from business logic

**Structure:**
```
genesys_to_snowflake/
â”œâ”€â”€ intraday.py                          # DAG definitions only
â””â”€â”€ src/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ main.py                          # Business logic functions
    â”œâ”€â”€ config.py                        # Configuration data
    â”œâ”€â”€ csc_alert_management.py          # Alert processing logic
    â””â”€â”€ construct_external_contact.py    # Helper functions
```

**Benefits:**
- âœ… **Separation of Concerns**: DAG orchestration vs business logic
- âœ… **Testability**: Can test business logic independently
- âœ… **Reusability**: Business logic can be imported by multiple DAGs
- âœ… **Maintainability**: Changes to business logic don't require DAG file edits
- âœ… **Clearer Responsibilities**: Each file has a single, clear purpose

**File Purposes:**
- **intraday.py**: DAG structure, task dependencies, scheduling
- **src/main.py**: Core functions that do the actual work
- **src/config.py**: Endpoint definitions, constants
- **src/csc_alert_management.py**: Alert-specific business logic
- **src/helpers.py**: Utility functions, data transformations

### 3. Configuration-Driven Task Generation

**Pattern**: Tasks dynamically created from configuration dictionary

**Implementation:**
```python
# config.py
endpoints = {
    "user": {
        "s3_key": "users",
        "table_name": "users",
        "fetch_method": ApiFunctions.FetchUsers,
        "schedule": ["nightly"],
    },
    # ... 20+ more endpoints
}

# intraday.py
for endpoint, config in endpoints.items():
    if 'nightly' in config['schedule']:
        to_s3 = genesys_to_s3_task.override(task_id=f"genesys_to_s3_{endpoint}")(
            s3_key=config['s3_key'],
            table_name=config['table_name'],
            fetch_method=config['fetch_method']
        )
```

**Benefits:**
- âœ… **Single Source of Truth**: All endpoint configurations in one place
- âœ… **Easy to Extend**: Add new endpoint = add one dictionary entry
- âœ… **Reduces Duplication**: One task definition serves 20+ endpoints
- âœ… **Maintainability**: Change task logic once, applies to all endpoints
- âœ… **Scalability**: Can easily add 100+ endpoints without code changes

**Pattern Breakdown:**
1. **Configuration File** (`config.py`): Defines all endpoints
2. **Task Template** (`@task` function): Defines task behavior
3. **Dynamic Generation** (loop): Creates tasks from configuration
4. **Override Pattern** (`.override(task_id=...)`): Unique task IDs per endpoint

### 4. Task Override Pattern

**Pattern**: Single task definition reused with dynamic task IDs

**Implementation:**
```python
@task
def genesys_to_s3_task(s3_key: str, table_name: str, fetch_method: callable):
    genesys_to_s3(s3_key=s3_key, table_name=table_name, fetch_method=fetch_method)

# Reuse with different task IDs
task1 = genesys_to_s3_task.override(task_id="genesys_to_s3_users")(
    s3_key="users", table_name="users", fetch_method=ApiFunctions.FetchUsers
)

task2 = genesys_to_s3_task.override(task_id="genesys_to_s3_queues")(
    s3_key="queues", table_name="queues", fetch_method=ApiFunctions.FetchRoutingQueues
)
```

**Benefits:**
- âœ… **DRY Principle**: Don't Repeat Yourself
- âœ… **Single Definition**: One task definition for all endpoints
- âœ… **Unique Task IDs**: Each task has its own ID for Airflow UI
- âœ… **Easier Updates**: Change task logic in one place
- âœ… **Consistent Behavior**: All endpoints use same task logic

**Common Mistakes to Avoid:**
- âŒ **Forgetting `.override()`**: Task IDs would collide
- âŒ **Not calling the task**: Must invoke with `(params)`
- âŒ **Reusing task variable**: Creates only one task instance

### 5. Environment-Based Scheduling

**Pattern**: Schedule intervals determined by environment variable

**Implementation:**
```python
env = Variable.get("env")
hourly_schedule = "0 6-21 * * *" if env == "prod" else None
nightly_schedule = "0 2 * * *" if env == "prod" else None
realtime_schedule = "*/5 6-18 * * *" if env == "prod" else None

@dag(
    dag_id="genesys_to_snowflake_nightly",
    schedule_interval=nightly_schedule,
    ...
)
```

**Benefits:**
- âœ… **Environment Isolation**: Dev/staging won't run on schedule
- âœ… **Manual Testing**: Non-prod environments use manual triggers
- âœ… **Same Codebase**: Identical code across all environments
- âœ… **Simplified Deployment**: No conditional DAG files
- âœ… **Prevents Accidents**: Can't accidentally trigger prod schedule in dev

**Alternative Pattern** (for complex scheduling):
```python
from airflow.models import Variable

def get_schedule(dag_name: str) -> str:
    """Get schedule based on environment and DAG name."""
    env = Variable.get("env")
    if env != "prod":
        return None

    schedules = {
        "nightly": "0 2 * * *",
        "hourly": "0 6-21 * * *",
        "realtime": "*/5 6-18 * * *"
    }
    return schedules.get(dag_name)
```

### 6. Explicit Logger Setup

**Pattern**: Each module creates its own logger

**Implementation:**
```python
import logging
logger = logging.getLogger(__name__)

@task
def my_task():
    logger.info("Starting task execution")
    logger.debug(f"Processing endpoint: {endpoint}")
    logger.error(f"Failed to process: {e}", exc_info=True)
```

**Benefits:**
- âœ… **Module Identification**: Logger name shows source module
- âœ… **Python Best Practice**: Standard logging pattern
- âœ… **Filterable Logs**: Can filter by logger name
- âœ… **Debug Support**: Can set different log levels per module
- âœ… **Stack Traces**: `exc_info=True` includes full traceback

**Log Levels:**
- `DEBUG`: Detailed diagnostic info (verbose)
- `INFO`: Confirmation that things are working
- `WARNING`: Something unexpected happened
- `ERROR`: Error occurred, but task continued
- `CRITICAL`: Serious error, task cannot continue

### 7. Type Hints for All Task Functions

**Pattern**: Every task function has type-annotated parameters

**Implementation:**
```python
from typing import Callable

@task
def genesys_to_s3_task(
    s3_key: str,
    table_name: str,
    fetch_method: Callable[[str], dict]
) -> None:
    """
    Task to fetch data from Genesys and store in S3.

    Args:
        s3_key: S3 key prefix for uploaded files
        table_name: Snowflake table name
        fetch_method: Function to fetch data from Genesys API

    Returns:
        None
    """
    genesys_to_s3(s3_key=s3_key, table_name=table_name, fetch_method=fetch_method)
```

**Benefits:**
- âœ… **Type Safety**: Catch type errors before runtime
- âœ… **IDE Support**: Better autocomplete and refactoring
- âœ… **Self-Documenting**: Parameter types are immediately clear
- âœ… **mypy Compatible**: Can use static type checker
- âœ… **Easier Debugging**: Type mismatches caught early

**Advanced Type Hints:**
```python
from typing import Callable, Dict, List, Optional, Union

@task
def process_data(
    data: List[Dict[str, any]],
    config: Optional[Dict[str, str]] = None,
    callback: Callable[[Dict], None] = None
) -> Union[Dict, None]:
    pass
```

### 8. Class-Based Callback Management

**Pattern**: Callbacks organized in a centralized class

**Implementation:**
```python
# common/custom_callbacks/custom_callbacks.py
class AirflowCallback:
    def __init__(self):
        self.logger = logging.getLogger(__name__)

    def on_success_callback(self, context):
        dag_id = context['dag'].dag_id
        task_id = context['task_instance'].task_id
        self.logger.info(f"Task succeeded: {dag_id}.{task_id}")
        # Send metrics to monitoring system

    def on_failure_callback(self, context):
        dag_id = context['dag'].dag_id
        task_id = context['task_instance'].task_id
        exception = context.get('exception')
        self.logger.error(f"Task failed: {dag_id}.{task_id}: {exception}")
        # Send alert to Slack/PagerDuty

# In DAG file
from common.custom_callbacks.custom_callbacks import AirflowCallback as cb

default_args = {
    "on_success_callback": [cb().on_success_callback],
    "on_failure_callback": [cb().on_failure_callback]
}
```

**Benefits:**
- âœ… **Encapsulation**: All callback logic in one place
- âœ… **Shared State**: Class can maintain configuration, connections
- âœ… **Multiple Callbacks**: List format allows `[cb1, cb2]`
- âœ… **Testability**: Easy to mock class methods
- âœ… **Reusability**: Same callback class across all DAGs

### 9. Simplified Data Loading (No Separate Snowflake Tasks)

**Pattern**: Integrate Snowflake loading into data extraction task

**Legacy (Airflow 1.x) - 2 Tasks:**
```python
genesys_to_s3_task >> s3_to_snowflake_task
```

**Modern (Airflow 2.x) - 1 Task:**
```python
@task
def genesys_to_s3_task(s3_key: str, table_name: str, fetch_method: callable):
    # 1. Fetch from Genesys API
    data = fetch_method()

    # 2. Upload to S3
    s3_hook.upload(data, s3_key)

    # 3. Trigger Snowflake COPY (integrated)
    snowflake_hook.copy_from_s3(s3_key, table_name)
```

**Benefits:**
- âœ… **Simpler DAG Graph**: Fewer tasks to monitor
- âœ… **Atomic Operation**: Data extraction and loading happen together
- âœ… **Faster Execution**: No task overhead between S3 and Snowflake
- âœ… **Fewer Failure Points**: One task vs two tasks
- âœ… **Easier Retry Logic**: Retry entire pipeline together

### 10. Ternary Expressions for Conditional Configuration

**Pattern**: Use ternary operators for simple conditionals

**Legacy:**
```python
if env == "local":
    schedule = None
elif env == "staging":
    schedule = None
elif env == "prod":
    schedule = "0 2 * * *"
```

**Modern:**
```python
schedule = "0 2 * * *" if env == "prod" else None
```

**Benefits:**
- âœ… **Concise**: 5 lines â†’ 1 line
- âœ… **Readable**: Intent is immediately clear
- âœ… **Pythonic**: Idiomatic Python pattern
- âœ… **Less Error-Prone**: Fewer branches to maintain

**When to Use:**
- âœ… Simple binary conditions
- âœ… Short result values
- âŒ Complex multi-condition logic (use if/elif/else)
- âŒ Long expressions (readability suffers)

---

## Appendix

### A. Complete File Tree Comparison

#### Legacy (Airflow 1.x)
```
dags/
â””â”€â”€ genesys_to_snowflake.py               (246 lines)

config/
â””â”€â”€ genesys_to_snowflake/
    â”œâ”€â”€ config.py
    â”œâ”€â”€ functions.py
    â””â”€â”€ csc_alert_management.py

plugins/
â”œâ”€â”€ operators/
â”‚   â”œâ”€â”€ on_success_callback.py
â”‚   â””â”€â”€ on_failure_callback.py
â””â”€â”€ hooks/
    â”œâ”€â”€ s3_upload_hook.py
    â””â”€â”€ snowflake_custom.py
```

#### Modern (Airflow 2.x)
```
dags/
â””â”€â”€ genesys_to_snowflake/
    â”œâ”€â”€ intraday.py                       (139 lines)
    â””â”€â”€ src/
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ main.py
        â”œâ”€â”€ config.py
        â”œâ”€â”€ csc_alert_management.py
        â””â”€â”€ construct_external_contact.py

common/
â”œâ”€â”€ custom_callbacks/
â”‚   â””â”€â”€ custom_callbacks.py
â””â”€â”€ custom_hooks/
    â”œâ”€â”€ custom_s3_hook.py
    â””â”€â”€ custom_snowflake_hook.py
```

### B. Migration Timeline Estimate

For a DAG similar to this one (246 lines, 3 sub-DAGs, 20+ endpoints):

| Phase | Duration | Effort |
|-------|----------|--------|
| **Assessment** | 2-4 hours | Review code, document dependencies |
| **Implementation** | 4-8 hours | Refactor to TaskFlow API, restructure files |
| **Testing** | 4-6 hours | Unit tests, integration tests, validation |
| **Staging Deployment** | 1-2 days | Monitor staging runs |
| **Production Deployment** | 1 day | Parallel run, cutover |
| **Post-Migration Monitoring** | 3-5 days | Ensure stability |
| **Total** | **10-15 hours coding** | **1-2 weeks calendar time** |

### C. Common Migration Pitfalls

1. **Forgetting to call DAG function**
   ```python
   # âŒ WRONG - DAG never instantiated
   @dag(...)
   def my_dag():
       pass

   # âœ… CORRECT
   dag_instance = my_dag()
   ```

2. **Not using `.override()` for dynamic task IDs**
   ```python
   # âŒ WRONG - All tasks have same ID
   for endpoint in endpoints:
       task = my_task(endpoint)  # task_id collisions!

   # âœ… CORRECT
   for endpoint in endpoints:
       task = my_task.override(task_id=f"task_{endpoint}")(endpoint)
   ```

3. **Mixing legacy and modern patterns**
   ```python
   # âŒ WRONG - Inconsistent patterns
   @dag(...)
   def my_dag():
       @task
       def task1():
           pass

       task2 = PythonOperator(...)  # Don't mix!
   ```

4. **Forgetting to remove `provide_context=True`**
   ```python
   # âŒ WRONG - Not valid in Airflow 2.x
   @task(provide_context=True)
   def my_task(**context):
       pass

   # âœ… CORRECT - Context automatic
   @task
   def my_task():
       # Access context if needed via ti, dag_run, etc.
       pass
   ```

### D. Useful Airflow CLI Commands

```bash
# List all DAGs
airflow dags list

# Check for import errors
airflow dags list-import-errors

# Show DAG structure (visual)
airflow dags show genesys_to_snowflake_nightly

# Test DAG (doesn't write to database)
airflow dags test genesys_to_snowflake_nightly 2023-11-20

# Test specific task
airflow tasks test genesys_to_snowflake_nightly genesys_to_s3_users 2023-11-20

# Trigger DAG manually
airflow dags trigger genesys_to_snowflake_nightly

# Pause/Unpause DAG
airflow dags pause genesys_to_snowflake_nightly
airflow dags unpause genesys_to_snowflake_nightly

# Backfill DAG
airflow dags backfill genesys_to_snowflake_nightly \
    --start-date 2023-11-20 \
    --end-date 2023-11-25

# Check task state
airflow tasks state genesys_to_snowflake_nightly genesys_to_s3_users 2023-11-20
```

### E. Further Reading

**Official Airflow Documentation:**
- [Airflow 2.0 Migration Guide](https://airflow.apache.org/docs/apache-airflow/stable/upgrading-to-2.html)
- [TaskFlow API Tutorial](https://airflow.apache.org/docs/apache-airflow/stable/tutorial/taskflow.html)
- [DAG Decorator Reference](https://airflow.apache.org/docs/apache-airflow/stable/howto/create-dag-decorator.html)

**Best Practices:**
- [Airflow Best Practices](https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html)
- [Testing DAGs](https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html#testing-a-dag)

**Provider Packages:**
- [Provider Packages Index](https://airflow.apache.org/docs/#providers-packages-docs-apache-airflow-providers-index-html)
- [Amazon AWS Provider](https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/index.html)
- [Snowflake Provider](https://airflow.apache.org/docs/apache-airflow-providers-snowflake/stable/index.html)

---

## Conclusion

This migration successfully modernized the Genesys to Snowflake DAG from Airflow 1.10.x to 2.x, achieving:

âœ… **43.5% code reduction** (246 â†’ 139 lines)
âœ… **TaskFlow API adoption** (eliminated 30+ `PythonOperator` instances)
âœ… **Modular structure** (1 file â†’ 5 files with clear separation of concerns)
âœ… **Type safety** (all task functions have type hints)
âœ… **Simplified orchestration** (removed redundant Snowflake tasks)
âœ… **Improved maintainability** (configuration-driven task generation)
âœ… **Zero data flow changes** (preserved all business logic)

### Key Takeaways

1. **TaskFlow API is essential**: Biggest impact on code quality and maintainability
2. **Modular structure pays dividends**: Separation of DAG definition from business logic
3. **Configuration-driven is scalable**: 20+ tasks from single config file
4. **Custom hooks are valid**: No requirement to migrate to provider packages
5. **Incremental migration works**: Can migrate DAGs one at a time
6. **Type hints improve quality**: Better IDE support and earlier error detection

### Recommended Next Steps

For teams planning similar migrations:

1. **Start with smallest DAG**: Build confidence with low-risk migration
2. **Create migration template**: Reuse patterns across multiple DAGs
3. **Parallel run initially**: Run old and new versions side-by-side
4. **Monitor closely**: Watch for behavioral differences
5. **Document learnings**: Share patterns and pitfalls with team
6. **Celebrate wins**: Recognize team effort and improved codebase

---

**Document Version**: 1.0
**Last Updated**: 2025-10-28
**Airflow Versions**: 1.10.x â†’ 2.x
**Status**: âœ… Production-Ready Migration Pattern

