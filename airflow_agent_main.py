"""
âš ï¸  DEPRECATED: This file is deprecated and will be removed in a future version.

Please use the new standardized entry point instead:

    python main.py --config yaml_files/airflow_agent_options.yaml

For more information, see:
- README.md (updated usage instructions)
- PROJECT_STRUCTURE.md (new structure documentation)

The new structure provides:
- Standardized Python package layout (main.py â†’ src/)
- YAML-based configuration
- Framework-agnostic orchestration
- Better maintainability

This file will continue to work during the transition period.
"""

from dotenv import load_dotenv

load_dotenv()
import asyncio
import os
import signal
import sys
import time
from pathlib import Path

load_dotenv(Path(__file__).resolve().parent / ".env.airflow", override=True)
from claude_agent_sdk import (
    ClaudeSDKClient,
    ClaudeAgentOptions,
    AgentDefinition,
    AssistantMessage,
    ResultMessage,
    TextBlock,
    ToolUseBlock,
    ThinkingBlock,
    ToolResultBlock,
    UserMessage,
    SystemMessage,
)
from util.helpers import load_markdown_for_prompt, display_message, file_path_creator
from util.log_set import log_config, logger

# Print deprecation warning
display_message("\n[yellow]âš ï¸  DEPRECATION WARNING:[/yellow]")
display_message("[yellow]This script (airflow_agent_main.py) is deprecated.[/yellow]")
display_message(
    "[yellow]Please use: python main.py --config yaml_files/airflow_agent_options.yaml[/yellow]"
)
display_message("[yellow]See README.md for updated usage instructions.[/yellow]\n")


class ConversationSession:
    """Maintains a single conversation session with Claude."""

    def __init__(
        self,
        options: ClaudeAgentOptions = None,
        project_dir: Path = Path(__file__).resolve().parent,
    ):
        self.options = options
        self.client = ClaudeSDKClient(self.options)
        self.turn_count = 0
        self.project_dir = project_dir
        self.should_exit = False

    async def start(self):
        """Start the conversation session with proper interrupt handling."""
        try:
            await self.client.connect()
            display_message(
                "Starting Airflow agent session. Claude will remember context."
            )
            display_message(
                "Commands: 'exit' to quit, 'interrupt' to stop current task, 'new' for new session"
            )

            display_message("What would you like to do?\n")
            display_message("  1. ğŸ†• Start a new Apache Airflow DAG project")
            display_message("  2. ğŸ“¦ Start a legacy DAG migration")

            while True:
                user_input = input(f"\n[Turn {self.turn_count + 1}] You: ")

                if user_input.lower() == "exit":
                    break
                elif user_input.lower() == "interrupt":
                    await self.client.interrupt()
                    display_message("Task interrupted!")
                    continue
                elif user_input.lower() == "1":
                    await self.new_dag_flow_start()
                elif user_input.lower() == "2":
                    await self.migrate_dag_flow_start()
                elif user_input.lower() == "new":
                    # Disconnect and reconnect for a fresh session
                    await self.client.disconnect()
                    await self.client.connect()
                    self.turn_count = 0
                    display_message(
                        "Started new conversation session (previous context cleared)"
                    )
                    continue
                else:
                    # Send regular message - Claude remembers all previous messages in this session
                    try:
                        await self.client.query(user_input)
                        self.turn_count += 1

                        # Process response
                        display_message(f"[Turn {self.turn_count}] Claude:")
                        async for message in self.client.receive_response():
                            display_message(message)

                        display_message("")  # New line after response

                    except KeyboardInterrupt:
                        display_message("\n\nResponse interrupted by user!")
                        logger.error("Response interrupted by user (KeyboardInterrupt)")
                        break
                    except asyncio.CancelledError as e_async:
                        logger.error(f"Task was cancelled. {e_async}")
                        display_message("\n\nTask was cancelled.")
                        break
                    except Exception as e:
                        logger.error(f"Error processing message: {e}")
                        display_message(f"\nError: {e}")
                        # Continue the loop to allow recovery

        except KeyboardInterrupt:
            display_message("\n\nSession interrupted by user.")
        except asyncio.CancelledError:
            display_message("\n\nAsync session cancelled.")
        except Exception as e:
            logger.error(f"Unexpected error in session: {e}")
            display_message(f"\nUnexpected error: {e}")
        finally:
            # Always attempt cleanup
            try:
                await self.client.disconnect()
            except Exception as e:
                logger.error(f"Error during disconnect: {e}")

            display_message(f"\nConversation ended after {self.turn_count} turns.")

    async def new_dag_flow_start(self):
        """
        Start of flow to invoke the DAG Orchestrator for greenfield DAG creation.
        Collects requirements following the Intake & Clarify phase from orchestrator workflow.
        """
        project_path = Path(self.project_dir)
        project_path.mkdir(parents=True, exist_ok=True)
        display_message(f"[dim]ğŸ“ Output directory: {project_path.absolute()}[/dim]\n")

        display_message(
            "[bold cyan]ğŸ†• New Airflow DAG Creation - Requirements Gathering[/bold cyan]\n"
        )
        display_message("[dim]Please provide the following information:[/dim]\n")

        # Collect requirements following orchestrator's Intake & Clarify phase
        dag_name = input("DAG/Pipeline Name (e.g., 'salesforce_daily'): ").strip()

        display_message("\n[bold]Business Objective & Data Flow:[/bold]")
        business_objective = input(
            "Business objective (what problem does this solve?): "
        ).strip()
        data_source = input(
            "Data source(s) (e.g., 'Enterprise SaaS API', 'dbt', '3rd Party REST API'): "
        ).strip()
        data_destination = input(
            "Data destination (e.g., 'S3 â†’ Snowflake external tables', 'S3 â†’ raw tables', SaaS application): "
        ).strip()

        display_message("\n[bold]Scheduling & Environment:[/bold]")
        schedule_cadence = input(
            "Schedule cadence (e.g., 'daily 1am', 'hourly', '@daily', 'None' for manual): "
        ).strip()
        environment = (
            input("Target environment (local/staging/prod) [default: local]: ").strip()
            or "local"
        )

        display_message("\n[bold]Requirements & Constraints:[/bold]")
        downstream_consumers = (
            input(
                "Downstream consumers (e.g., 'DBT models', 'BI dashboards', 'None') [optional]: "
            ).strip()
            or "None specified"
        )
        special_requirements = (
            input(
                "Special requirements (e.g., 'rate limiting', 'batching', 'large datasets') [optional]: "
            ).strip()
            or "Standard patterns"
        )
        sla_requirements = (
            input(
                "SLA requirements (e.g., 'Complete within 2 hours', 'None') [optional]: "
            ).strip()
            or "None specified"
        )

        # Build structured request following orchestrator expectations
        new_dag_request = f"""
Create a new Apache Airflow 2 DAG from scratch (greenfield development).

## Project Requirements

**Pipeline Name:** `{dag_name}`

**Business Objective:**
{business_objective}

**Data Flow:**
- **Source:** {data_source}
- **Destination:** {data_destination}
- **Downstream Consumers:** {downstream_consumers}

**Scheduling & Environment:**
- **Schedule:** {schedule_cadence}
- **Target Environment:** {environment}
- **SLA:** {sla_requirements}

**Special Requirements:**
{special_requirements}

## Implementation Guidance

Follow the standards in `airflow/airflow_CLAUDE.md`:
1. Use the standardized directory structure: `dags/{dag_name}/src/main.py`
2. Consider using `_dag_template/` or `_dag_taskflow_template/` as starting point
3. Implement heartbeat-safe code (no I/O at module scope)
4. Apply environment-aware configuration using `Variable.get("environment", default_var="local")`
5. Use appropriate hooks/operators from `common/` when available
6. Include proper type hints, docstrings, error handling, and logging
7. Implement batching for large datasets (default 250,000 records)
8. Follow the data pipeline pattern: Airflow fetches â†’ S3, DBT handles normalization

## Orchestration Flow

Please follow your standard operating loop:
1. **Intake & Clarify:** Review requirements above, identify any gaps or assumptions
2. **Plan & Decompose:** Design directory structure, module boundaries, validation strategy
3. **Delegate Work:** Assign to @dag-developer for implementation
4. **Integrate & Iterate:** Review output, request revisions if needed
5. **Validate & Close:** Engage @code-reviewer for final quality gate

Ensure the final deliverable is production-ready and passes all quality gates.
"""

        # Display summary
        display_message("\n" + "=" * 60)
        display_message(
            "[bold cyan]ğŸš€ Starting New Airflow DAG Creation Orchestrator[/bold cyan]\n"
        )
        display_message(f"[bold]Pipeline Name:[/bold] {dag_name}")
        display_message(f"[bold]Objective:[/bold] {business_objective}")
        display_message(f"[bold]Data Source:[/bold] {data_source}")
        display_message(f"[bold]Data Destination:[/bold] {data_destination}")
        display_message(f"[bold]Schedule:[/bold] {schedule_cadence}")
        display_message(f"[bold]Environment:[/bold] {environment}")
        display_message(f"[bold]SLA:[/bold] {sla_requirements}")
        display_message("=" * 60 + "\n")

        # Invoke orchestrator with structured request
        await self.airflow_app_orchestrator(new_dag_request)

    async def migrate_dag_flow_start(self):
        """
        Start of flow to invoke the DAG Orchestrator that migrates a DAG from Airflow 1.0 to 2.0.

        """
        start = time.perf_counter()
        project_path = Path(self.project_dir)
        project_path.mkdir(parents=True, exist_ok=True)
        display_message(f"[dim]ğŸ“ Output directory: {project_path.absolute()}[/dim]\n")

        # Get user requirements
        # legacy_dag_path = input("Path to legacy DAG file: ")

        legacy_dag_path = (
            "aptive_github/data-airflow-legacy/dags/invoca_to_snowflake.py"
        )
        file = file_path_creator(legacy_dag_path)
        # new_dag_name = input("New DAG name (leave blank to use same name): ")
        new_dag_name = "invoca_to_snowflake"

        migration_request = f"""
Migrate an Airflow 1.0 DAG to Airflow 2.0 with full modernization.

**Legacy DAG Path:** {file}
**New DAG Name:** {new_dag_name if new_dag_name else "Use original name"}
"""

        display_message(
            "[bold cyan]ğŸš€ Starting Airflow DAG creation orchestrator...[/bold cyan]\n"
        )
        display_message(f"[bold]Legacy DAG:[/bold] {file}")
        display_message(
            f"[bold]New DAG Name:[/bold] {new_dag_name if new_dag_name else '(same as original)'}\n"
        )

        # Combine orchestrator instructions with specific creation request

        await self.airflow_app_orchestrator(migration_request)

    async def airflow_app_orchestrator(self, request: str):
        """
        Orchestrator that creates a airflow web app with random styling.
        Loads prompts from markdown files via load_markdown_for_prompt().
        """
        try:
            main_prompt = load_markdown_for_prompt(
                "prompts/airflow_prompts/airflow-orchestrator-v2.md"
            )
            display_message(f"main_prompt: {main_prompt}")

            combined_prompt = f"{main_prompt}\n\n{request}"
            # Create project directories
            start_time = time.perf_counter()
            project_path = self.project_dir
            project_path.mkdir(parents=True, exist_ok=True)
            display_message(
                f"[dim]ğŸ“ Created project directory at: {project_path.absolute()}[/dim]\n"
            )
            display_message(
                "[dim]ğŸ’¡ Note: Agent will create its own directory structure[/dim]\n"
            )

            # Load markdown prompt files

            display_message(
                "[bold cyan]ğŸš€ Starting airflow app orchestrator with markdown prompts...[/bold cyan]\n"
            )

            # Metrics tracking
            # NOTE: message_count tracks individual messages in the response stream
            # turn_count tracks SDK agentic loop turns (limited by max_turns in ClaudeAgentOptions)
            # These are different: one turn can produce multiple messages (thinking, tool use, text, etc.)
            message_count = 0  # Total messages received (excluding StreamEvents)
            turn_count = (
                0  # Agentic loop cycles (AssistantMessage = Claude's turn response)
            )
            tool_use_count = 0
            thinking_count = 0
            text_block_count = 0
            files_created = []
            message_times = []  # Time per message
            iteration_costs = []

            async with self.client:

                # Use the already-connected client directly (no context manager needed)
                await self.client.query(prompt=combined_prompt)

                display_message(
                    f"[bold blue]{'='*60}[/bold blue]\n"
                    f"[bold yellow]ğŸ”„ Starting orchestration...[/bold yellow]\n"
                    f"[bold blue]{'='*60}[/bold blue]\n"
                )
                total_messages = 0  # Includes StreamEvents for display purposes

                async for message in self.client.receive_response():
                    total_messages += 1
                    logger.debug(message)
                    # Skip StreamEvent messages entirely - they are logged to file only
                    display_message(message, print_raw=True)

                    message_count += 1
                    message_start_time = time.perf_counter()

                    # Track turn count (each AssistantMessage = one agentic loop turn)
                    if isinstance(message, AssistantMessage):
                        self.turn_count += 1

                        display_message(message, iteration=self.turn_count)

                    # Display message header
                    display_message(
                        f"\n[bold magenta]{'â”€'*60}[/bold magenta]\n"
                        f"[bold cyan]ğŸ“ MESSAGE {message_count} | Turn {self.turn_count} | "
                        f"Type: {message.__class__.__name__}[/bold cyan]\n"
                        f"[bold magenta]{'â”€'*60}[/bold magenta]"
                    )

                    # Display the message with debugging context
                    display_message(message, iteration=message_count)

                    # Track message-specific metrics
                    if isinstance(message, AssistantMessage):
                        for block in message.content:
                            if isinstance(block, TextBlock):
                                text_block_count += 1
                            elif isinstance(block, ThinkingBlock):
                                thinking_count += 1
                            elif isinstance(block, ToolUseBlock):
                                tool_use_count += 1
                                if block.name == "Write" and "file_path" in block.input:
                                    files_created.append(block.input["file_path"])

                    # Calculate message processing time
                    message_end_time = time.perf_counter()
                    message_duration = message_end_time - message_start_time
                    message_times.append(message_duration)

                    # Display message metrics
                    display_message(
                        f"[dim]â±ï¸  Message {message_count} took: {message_duration:.3f}s[/dim]"
                    )

                    if isinstance(message, ResultMessage):
                        cost = float(message.total_cost_usd or 0)
                        iteration_costs.append(cost)

                        # Calculate final metrics
                        end_time = time.perf_counter()
                        total_duration = end_time - start_time
                        avg_message_time = (
                            sum(message_times) / len(message_times)
                            if message_times
                            else 0
                        )

                        display_message(
                            f"\n[bold green]{'='*60}[/bold green]\n"
                            f"[bold green]âœ… airflow app creation complete![/bold green]\n"
                            f"[bold green]{'='*60}[/bold green]\n"
                        )

                        display_message(
                            f"\n[bold yellow]ğŸ“Š ORCHESTRATION METRICS:[/bold yellow]"
                        )
                        display_message(f"[bold cyan]{'â”€'*60}[/bold cyan]")
                        display_message(
                            f"[bold white]Agentic Loop Metrics:[/bold white]"
                        )
                        display_message(
                            f"   â€¢ SDK Turns (agentic loops): [bold]{self.turn_count}[/bold] "
                            f"(max_turns={self.options.max_turns if hasattr(self.options, 'max_turns') else 'unlimited'})"
                        )
                        display_message(
                            f"   â€¢ Total messages processed: [bold]{message_count}[/bold]"
                        )
                        display_message(
                            f"   â€¢ Messages per turn: [bold]{message_count/turn_count if turn_count > 0 else 0:.1f}[/bold]"
                        )

                        display_message(
                            f"\n[bold white]Message Processing Times:[/bold white]"
                        )
                        display_message(
                            f"   â€¢ Average time/message: [bold]{avg_message_time:.3f}s[/bold]"
                        )
                        display_message(
                            f"   â€¢ Fastest message: [bold]{min(message_times):.3f}s[/bold]"
                        )
                        display_message(
                            f"   â€¢ Slowest message: [bold]{max(message_times):.3f}s[/bold]"
                        )

                        display_message(f"\n[bold white]Content Metrics:[/bold white]")
                        display_message(f"   â€¢ Text blocks: {text_block_count}")
                        display_message(f"   â€¢ Thinking blocks: {thinking_count}")
                        display_message(f"   â€¢ Tool uses: {tool_use_count}")
                        display_message(f"   â€¢ Files created: {len(files_created)}")

                        display_message(
                            f"\n[bold white]Cost & Time Metrics:[/bold white]"
                        )
                        if cost > 0:
                            display_message(
                                f"   â€¢ Total cost: [bold yellow]${cost:.6f}[/bold yellow]"
                            )
                            if turn_count > 0:
                                display_message(
                                    f"   â€¢ Cost per turn: [bold]${cost/turn_count:.6f}[/bold]"
                                )
                            if message_count > 0:
                                display_message(
                                    f"   â€¢ Cost per message: [bold]${cost/message_count:.6f}[/bold]"
                                )
                        display_message(
                            f"   â€¢ Total duration: [bold green]{total_duration:.2f}s[/bold green]"
                        )

                        display_message(f"\n[bold white]Token Usage:[/bold white]")
                        if hasattr(message, "input_tokens") and message.input_tokens:
                            display_message(
                                f"   â€¢ Input tokens: {message.input_tokens:,}"
                            )
                        if hasattr(message, "output_tokens") and message.output_tokens:
                            display_message(
                                f"   â€¢ Output tokens: {message.output_tokens:,}"
                            )
                        if hasattr(message, "input_tokens") and hasattr(
                            message, "output_tokens"
                        ):
                            total_tokens = (message.input_tokens or 0) + (
                                message.output_tokens or 0
                            )
                            if total_tokens > 0:
                                display_message(f"   â€¢ Total tokens: {total_tokens:,}")

                        display_message(f"\n[bold cyan]{'â”€'*60}[/bold cyan]")
                        display_message(
                            f"\n[bold]ğŸ“ Project location:[/bold] {project_path.absolute()}"
                        )

                        if files_created:
                            display_message("[bold white]Files created:[/bold white]")
                            for i, file in enumerate(files_created, 1):
                                display_message(f"   {i}. {file}")

                        display_message(
                            f"\n[bold green]{'='*60}[/bold green]\n"
                            f"[bold green]â±ï¸  TOTAL TIME: {total_duration:.2f} seconds[/bold green]\n"
                            f"[bold green]ğŸ”„ SDK TURNS: {self.turn_count} | MESSAGES: {message_count}[/bold green]\n"
                            f"[bold green]{'='*60}[/bold green]\n"
                        )

        except Exception as ex:
            display_message(f"[bold red]Error:[/bold red] {ex}")
            logger.error(f"Unhandled exception: {ex}", exc_info=True)
            sys.exit(1)


if __name__ == "__main__":
    try:
        # logging setup
        CLAUDE_LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO")
        log_config.setup(CLAUDE_LOG_LEVEL)
        # Script location: /home/dev/claude_swarm/airflow_agent.py
        script_path = Path(__file__).resolve()

        # Project root: /home/dev (where CLAUDE.md is located)
        project_root = script_path.parent.parent  # /home/dev

        # Output directory: /home/dev/claude_swarm/generated_code
        output_dir = "data-airflow/dags"

        dev_airflow_dir = project_root

        display_message(
            "[bold magenta]â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—[/bold magenta]"
        )
        display_message(
            "[bold magenta]â•‘  Apache Airflow Claude Code Agent Swarm (Markdown Prompts)  â•‘[/bold magenta]"
        )
        display_message(
            "[bold magenta]â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•[/bold magenta]\n"
        )
        display_message(f"[dim]Project root (CLAUDE.md location): {project_root}[/dim]")
        display_message(f"[dim]Default output directory: {output_dir}[/dim]")
        display_message(f"[dim]Additional access: {dev_airflow_dir}[/dim]\n")

        # Load markdown prompt files \
        airflow_dev_prompt = load_markdown_for_prompt(
            "prompts/airflow_prompts/dag-developer.md"
        )
        migration_specialist = load_markdown_for_prompt(
            "prompts/airflow_prompts/migration-specialist.md"
        )
        code_reviewer_prompt = load_markdown_for_prompt(
            "prompts/airflow_prompts/airflow-code-reviewer.md"
        )
        orchestrator_agent = load_markdown_for_prompt("repo_CLAUDE.md")

        # Model Variable
        CLAUDE_MODEL = os.getenv("CLAUDE_MODEL", "sonnet")
        display_message(f"Using CLAUDE_MODEL env: {CLAUDE_MODEL}")

        # Build environment configuration from .env settings with sensible defaults
        default_airflow_root = project_root / "data-airflow"
        default_airflow_dags = default_airflow_root / "dags"
        default_legacy_root = project_root / "data-airflow-legacy"
        default_legacy_dags = default_legacy_root / "dags"

        env_defaults = {
            "AIRFLOW_HOME": str(project_root),
            "AIRFLOW__CORE__DAGS_FOLDER": str(default_airflow_dags),
            "AIRFLOW_2_DAGS_DIR": str(default_airflow_dags),
            "AIRFLOW_2_ROOT": str(default_airflow_root),
            "AIRFLOW_LEGACY_DAGS_DIR": str(default_legacy_dags),
            "AIRFLOW_LEGACY_ROOT": str(default_legacy_root),
        }

        env_config: dict[str, str] = {}
        for key, default_value in env_defaults.items():
            value = os.getenv(key, default_value)
            if value:
                env_config[key] = value

        python_path = os.getenv("PYTHONPATH")
        if python_path:
            env_config["PYTHONPATH"] = python_path

        env_config["PROJECT_ROOT"] = str(project_root)

        add_dirs = [
            path
            for path in [
                env_config.get("AIRFLOW_2_DAGS_DIR"),
                env_config.get("AIRFLOW_LEGACY_DAGS_DIR"),
            ]
            if path
        ]
        add_dirs = list(dict.fromkeys(add_dirs))  # Preserve order, drop duplicates

        # Determine session project directory from environment overrides
        project_dir_env = os.getenv("OUTPUT_DIR")
        if project_dir_env:
            project_dir_path = Path(project_dir_env)
            if not project_dir_path.is_absolute():
                project_dir_path = (project_root / project_dir_path).resolve()
        else:
            project_dir_path = (project_root / output_dir).resolve()

        display_message(f"[dim]Session project directory: {project_dir_path}[/dim]")
        env_config["OUTPUT_DIR"] = str(project_dir_path)

        # Construct Claude agent options using environment-backed configuration
        options = ClaudeAgentOptions(
            system_prompt={
                "type": "preset",
                "preset": "claude_code",
                "append": orchestrator_agent,
            },
            max_turns=35,
            model=CLAUDE_MODEL,
            setting_sources=[
                "project"
            ],  # This tells SDK to look for .claude/settings.json at project root
            cwd=str(output_dir),
            add_dirs=add_dirs,
            env=env_config,
            agents={
                "dag-developer": AgentDefinition(
                    description="Expert Airflow 2 developer for writing production-ready DAG code with type hints and best practices.",
                    prompt=airflow_dev_prompt,
                    tools=["Read", "Write", "Edit", "Bash", "Grep", "Glob"],
                    model="haiku",
                ),
                "migration-specialist": AgentDefinition(
                    description="Expert in migrating Airflow 1.0 DAGs to 2.0 with code modernization, breaking monolithic functions, and implementing clean code principles.",
                    prompt=migration_specialist,
                    tools=[
                        "Read",
                        "Write",
                        "Edit",
                        "Grep",
                        "Glob",
                        "mcp__migration__detect_legacy_imports",
                        "mcp__migration__detect_deprecated_parameters",
                        "mcp__migration__compare_dags",
                    ],
                    model="haiku",
                ),
                "airflow-code-reviewer": AgentDefinition(
                    description="Code review specialist for Airflow best practices, CLAUDE.md compliance, PEP 8, type hints, and production readiness.",
                    prompt=code_reviewer_prompt,
                    tools=[
                        "Read",
                        "Grep",
                        "Glob",
                        "Bash",
                        "mcp__migration__detect_legacy_imports",
                        "mcp__migration__detect_deprecated_parameters",
                    ],
                    model="haiku",
                ),
            },
            allowed_tools=[
                "Read",
                "Write",
                "Edit",
                "Bash",
                "Grep",
                "Glob",
                "mcp__migration__detect_legacy_imports",
                "mcp__migration__detect_deprecated_parameters",
                "mcp__migration__compare_dags",
            ],
            permission_mode="acceptEdits",
        )

        session = ConversationSession(options, project_dir=project_dir_path)

        def signal_handler(_signum, _frame):
            """Handle interrupt signals gracefully."""
            display_message("\n\nReceived interrupt signal. Shutting down...")
            session.should_exit = True
            sys.exit(0)

        signal.signal(signal.SIGINT, signal_handler)

        try:
            asyncio.run(session.start())
        except KeyboardInterrupt:
            display_message("\nInterrupted by user. Exiting...")
        except Exception as e:
            display_message(f"\nUnexpected error: {e}")
            logger.error(f"Unexpected error: {e}")
            sys.exit(1)
        finally:
            sys.exit(0)
    except Exception as ex:
        display_message(f"\nFatal error: {ex}")
        logger.error(f"Fatal error: {ex}")
        sys.exit(1)
